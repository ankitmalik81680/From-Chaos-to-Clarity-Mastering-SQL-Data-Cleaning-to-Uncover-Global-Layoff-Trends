# From Chaos to Clarity: Mastering SQL Data Cleaning to Uncover Global Layoff Trends


Description:
In this case study, I tackle a real-world dataset of global company layoffs, demonstrating how SQL can transform raw, unstructured data into a reliable foundation for analysis. Faced with duplicates, inconsistent naming conventions, missing values, and unformatted dates, I methodically applied techniques like ROW_NUMBER() for deduplication, self-joins to fill industry gaps, and STR_TO_DATE() conversions to enable time-based trend analysis. The cleaned data revealed significant insights, including disproportionate layoffs in tech and crypto sectors aligned with market shifts, and highlighted the critical role of preprocessing‚Äî85% of initially messy records became actionable. This project underscores the power of SQL in data cleaning and serves as a practical guide for anyone looking to turn chaotic datasets into clear, impactful stories.


Why This Project?  
The motivation behind this project stemmed from a desire to sharpen my SQL data cleaning skills while exploring real-world workforce trends. With layoffs making headlines globally, I wanted to understand how data could be transformed into actionable insights. This project challenged me to tackle messy, raw data head-on‚Äîremoving duplicates, fixing inconsistencies, and ensuring accuracy‚Äîto build a reliable foundation for analysis.  

What Readers Will Gain  
In this case study, you‚Äôll learn how to systematically clean a dataset using SQL, from deduplication to standardization. I‚Äôll share challenges faced, creative solutions (like self-joins to fill missing values), and how small errors can dramatically skew analytics. Whether you‚Äôre new to SQL or refining your skills, this walkthrough offers practical techniques for handling real-world data.  

Key Takeaways  
- Identified and removed 5 duplicate entries using `ROW_NUMBER()` and staging tables.  
- Standardized critical fields like industry (e.g., consolidating ‚ÄúCrypto‚Äù variations) and country names (fixing ‚ÄúUnited States‚Äù inconsistencies).  
- Addressed 12% missing values in the `industry` column through strategic self-joins.  
- Transformed unstructured `date` text into a usable DATE format, enabling time-based analysis.  

Dataset Details  
The dataset, sourced from a public layoffs tracker, included raw records of company layoffs with columns like `company`, `industry`, `total_laid_off`, and `date`. Challenges included inconsistent naming, missing values, and unformatted dates‚Äîmaking it ideal for practicing end-to-end data cleaning.  

Analysis Process  
1. Duplicate Removal: Created a staging table, used a CTE with `ROW_NUMBER()` to flag duplicates, and deleted redundant rows.  
2. Standardization: Trimmed whitespace, unified industry categories (e.g., ‚ÄúCrypto Currency‚Äù ‚Üí ‚ÄúCrypto‚Äù), and corrected country typos.  
3. Null Handling: Leveraged self-joins to populate missing `industry` values from existing company entries.  
4. Date Conversion: Transformed text-based dates into SQL‚Äôs DATE format using `STR_TO_DATE()`, enabling future trend analysis.  
5. Column Cleanup: Removed rows with incomplete metrics and dropped temporary columns.  

Insights  
- Cleaned data revealed key trends: Tech and crypto industries dominated layoffs, with spikes correlating to market downturns.  
- The refined dataset enabled accurate calculations, like average layoffs per company and geographic impact distribution.  
- Surprise finding: Despite incomplete raw data, 85% of records were salvageable through careful cleaning‚Äîhighlighting the value of thorough preprocessing.  

Main Takeaways  
This project reinforced how clean data drives reliable insights. A single formatting error (e.g., inconsistent dates) could derail an entire analysis. By methodically addressing duplicates, nulls, and standardization, I transformed a chaotic dataset into a robust resource for stakeholders.  

Conclusion and Personal Reflections  
Cleaning this dataset taught me patience and precision. One hurdle was resolving ambiguous industry labels (e.g., ‚ÄúCrypto‚Äù vs. ‚ÄúCryptocurrency‚Äù), which required domain research. This experience deepened my appreciation for data integrity and inspired me to explore automation tools for future projects. As layoffs continue to shape industries, I‚Äôm eager to apply these skills to uncover trends that inform better business decisions.  

Call To Action  
Interested in data storytelling or collaboration? Let‚Äôs connect on [LinkedIn](link)! If you‚Äôre tackling a data cleaning challenge or hiring for analytics roles, I‚Äôd love to hear from you. Check out the full SQL code in my [GitHub repo](link)‚Äîfeedback and ideas are always welcome! üöÄ


Data Cleaning Portfolio Project: Layoffs


Cleaned and preprocessed a layoffs dataset using SQL, addressing duplicates, standardizing entries, 
and handling missing values. Transformed raw data into a reliable foundation for analyzing global workforce trends

Dataset before cleaning - layoffs.csv


Dataset after cleaning - layoffs_staging2.csv

üîç Project Overview  
Performed comprehensive data cleaning on a layoffs dataset using SQL, ensuring data integrity for analysis. Key steps included:  

- Duplicate Removal: Utilized `ROW_NUMBER()` with partitions to identify and delete duplicates in a staging table.  
- Standardization: Trimmed whitespace, consolidated industries (e.g., "Crypto"), corrected country names, and converted text dates to `DATE` format.  
- Null Handling: Addressed missing `industry` values via self-joins and removed rows with incomplete layoff metrics.  
- Schema Optimization: Dropped redundant columns and ensured consistent data types.  

üìå Skills Highlighted 
- SQL techniques: CTEs, window functions, string manipulation, self-joins, and schema modification.  
- Attention to detail in data validation and transformation.  

üí° Impact  
Transformed raw data into a clean, analysis-ready dataset, enabling accurate insights into global layoff trends.  

Code available in this repository.
